# Data engineering capstone project

Overview 

I this project 4 datasets will be used. The main dataset will include data on immigration to the United States, and supplementary datasets will include data on airport codes, U.S. city demographics, and temperature data. You're also welcome to enrich the project with additional data if you'd like to set your project apart.

## Steps

1.- Scope the project and Gather Data

* Chosing the data two sources and one million rows.

* Explainig end use cases

2.- Explore and Assess the Data

* Data quality (dealing with, missing values, dupicate data, etc.)

* Document the data quality steps.

3.- Define the Data Model

* Explaining the data model.

* List the steps necessary to pipeline the data into the chosen data model.

4.- Run ETL to Model the Data.

* Creating data pipelines and model.

* Including a data dictionary.

* Running data quality checks to ensure the pipeline ran as expected.

5.- Complete Ptoject Write Up.

* What's the goal? What queries will you want to run? How would Spark or Airflow be incorporated? Why did you choose the model you chose?

* Clearly state the rationale for the choice of tools and technologies for the project.

* Document the steps of the process.

* Propose how often the data should be updated and why.

* Post your write-up and final data model in a GitHub repo.

* Include a description of how you would approach the problem differently under the following scenarios:

    * If the data was increased by 100x.

    * If the pipelines were run on a daily basis by 7am.

    * If the database needed to be accessed by 100+ people.